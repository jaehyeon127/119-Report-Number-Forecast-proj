import time
import pandas as pd
import xgboost
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
import joblib
from collections import Counter
from lightgbm import LGBMRegressor
from sklearn.linear_model import HuberRegressor, RidgeCV
from sklearn.ensemble import StackingRegressor
import warnings
import shap
import optuna

def evaluate_model(model, X, y, model_name, data_type="Test"):
    """
    ëª¨ë¸ì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    y_pred = model.predict(X)
    end_time = time.time()

    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    r2 = r2_score(y, y_pred)

    # ì¶”ê°€ í‰ê°€ ì§€í‘œ
    mape = np.mean(np.abs((y - y_pred) / np.maximum(y, 1))) * 100  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

    print(f"\n[{model_name} - {data_type} í‰ê°€]")
    print(f"  - ì˜ˆì¸¡ ì‹œê°„: {end_time - start_time:.4f} ì´ˆ")
    print(f"  - MAE: {mae:.4f}")
    print(f"  - RMSE: {rmse:.4f}")
    print(f"  - R2: {r2:.4f}")
    print(f"  - MAPE: {mape:.2f}%")

    # ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„
    print(f"  - ì‹¤ì œê°’ ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
    print(f"  - ì˜ˆì¸¡ê°’ ë²”ìœ„: {y_pred.min():.2f} ~ {y_pred.max():.2f}")
    print(f"  - ìŒìˆ˜ ì˜ˆì¸¡ê°’: {(y_pred < 0).sum()}ê°œ")

    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape, 'y_pred': y_pred}


def plot_predictions(y_true, y_pred, model_name,results_dir):
    """
    ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’, ì”ì°¨ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    plt.figure(figsize=(16, 8))

    # í•œê¸€ í°íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)
    try:
        plt.rc('font', family='Malgun Gothic')
    except:
        print("ê²½ê³ : 'Malgun Gothic' í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    plt.suptitle(f'{model_name} - ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„', fontsize=16)

    # 1. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„
    plt.subplot(2, 2, 1)
    plt.scatter(y_true, y_pred, alpha=0.6, s=10)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--', color='red', linewidth=2)
    plt.title('ì‹¤ì œê°’ vs. ì˜ˆì¸¡ê°’')
    plt.xlabel('ì‹¤ì œê°’')
    plt.ylabel('ì˜ˆì¸¡ê°’')
    plt.grid(True, alpha=0.3)

    # RÂ² ê°’ í‘œì‹œ
    r2 = r2_score(y_true, y_pred)
    plt.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=plt.gca().transAxes,
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

    # 2. ì”ì°¨ ë¶„í¬
    plt.subplot(2, 2, 2)
    residuals = y_true - y_pred
    sns.histplot(residuals, kde=True, bins=30)
    plt.title('ì”ì°¨ ë¶„í¬')
    plt.xlabel('ì”ì°¨ (ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)')
    plt.ylabel('ë¹ˆë„')
    plt.grid(True, alpha=0.3)

    # ì”ì°¨ í†µê³„ í‘œì‹œ
    plt.axvline(residuals.mean(), color='red', linestyle='--', alpha=0.7, label=f'í‰ê· : {residuals.mean():.2f}')
    plt.legend()

    # 3. ì˜ˆì¸¡ê°’ ë¶„í¬
    plt.subplot(2, 2, 3)
    plt.hist(y_true, bins=30, alpha=0.7, label='ì‹¤ì œê°’', color='blue')
    plt.hist(y_pred, bins=30, alpha=0.7, label='ì˜ˆì¸¡ê°’', color='orange')
    plt.title('ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ë¶„í¬')
    plt.xlabel('ê°’')
    plt.ylabel('ë¹ˆë„')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 4. ì”ì°¨ vs ì˜ˆì¸¡ê°’ (ì´ìƒì¹˜ íƒì§€)
    plt.subplot(2, 2, 4)
    plt.scatter(y_pred, residuals, alpha=0.6, s=10)
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)
    plt.title('ì”ì°¨ vs ì˜ˆì¸¡ê°’')
    plt.xlabel('ì˜ˆì¸¡ê°’')
    plt.ylabel('ì”ì°¨')
    plt.grid(True, alpha=0.3)

    plt.tight_layout(rect=[0, 0, 1, 0.94])
    os.makedirs(results_dir, exist_ok=True)  # 2. í•˜ë“œì½”ë”©ëœ 'results' ëŒ€ì‹  ì¸ìë¡œ ë°›ì€ results_dirë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    save_path = os.path.join(results_dir, f'{model_name}_prediction_analysis.png')  # 3. ì €ì¥ ê²½ë¡œë¥¼ ì¡°í•©í•©ë‹ˆë‹¤.
    plt.savefig(save_path, dpi=300, bbox_inches='tight')  # 4. ì¡°í•©ëœ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
    plt.close()

    print(f"âœ… ì˜ˆì¸¡ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥: results/{model_name}_prediction_analysis.png")


def plot_feature_importance(model, feature_names, model_name,results_dir):
    """
    í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    importance = model.feature_importances_
    df_importance = pd.DataFrame({'feature': feature_names, 'importance': importance})
    df_importance = df_importance.sort_values('importance', ascending=False)

    # ìƒìœ„ 30ê°œ í”¼ì²˜
    top_features = df_importance.head(30)

    plt.figure(figsize=(12, 10))
    try:
        plt.rc('font', family='Malgun Gothic')
        plt.rcParams['axes.unicode_minus'] = False
    except:
        print("ê²½ê³ : 'Malgun Gothic' í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    sns.barplot(x='importance', y='feature', data=top_features, palette='viridis')
    plt.title(f'{model_name} - ìƒìœ„ 30ê°œ í”¼ì²˜ ì¤‘ìš”ë„')
    plt.xlabel('ì¤‘ìš”ë„')
    plt.ylabel('í”¼ì²˜')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    os.makedirs(results_dir, exist_ok=True)
    save_path = os.path.join(results_dir, f'{model_name}_feature_importance.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë˜í”„ ì €ì¥: results/{model_name}_feature_importance.png")

    # í”¼ì²˜ ì¤‘ìš”ë„ ìƒìœ„ 20ê°œ ì¶œë ¥
    print(f"\nğŸ” ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜:")
    for i, (_, row) in enumerate(top_features.head(20).iterrows(), 1):
        print(f"  {i:2d}. {row['feature']:<40} : {row['importance']:.4f}")

        # í”¼ì²˜ ìœ í˜•ë³„ ì¤‘ìš”ë„ ë¶„ì„
    print(f"\nğŸ“Š í”¼ì²˜ ìœ í˜•ë³„ ì¤‘ìš”ë„ ë¶„ì„:")
    feature_types = {
        'accident_': 'êµí†µì‚¬ê³ ',
        'cat_': 'ì‹ ê³ ìœ í˜•',
        'subcat_': 'ì„¸ë¶€ìœ í˜•',
        'address_': 'ì§€ì—­',
        'population_': 'ì¸êµ¬',
        'ta_': 'ê¸°ì˜¨',
        'hm_': 'ìŠµë„',
        'ws_': 'í’ì†',
        'rn_': 'ê°•ìˆ˜',
        'alert_': 'ê¸°ìƒíŠ¹ë³´',
        'year': 'ì‹œê°„_ë…„',
        'month': 'ì‹œê°„_ì›”',
        'day': 'ì‹œê°„_ì¼',
        'dayofweek': 'ì‹œê°„_ìš”ì¼',
        'quarter': 'ì‹œê°„_ë¶„ê¸°',
        'is_weekend': 'ì‹œê°„_ì£¼ë§ì—¬ë¶€',
        'season_': 'ì‹œê°„_ê³„ì ˆ',
        'is_holiday': 'ì‹œê°„_ê³µíœ´ì¼ì—¬ë¶€'
    }

    # ê° í”¼ì²˜ê°€ ì–´ë–¤ ìœ í˜•ì— ì†í•˜ëŠ”ì§€ ë§¤í•‘
    feature_type_map = {}
    for col in feature_names:
        found = False
        for prefix, type_name in feature_types.items():
            if col.startswith(prefix):
                feature_type_map[col] = type_name
                found = True
                break
        if not found:
            feature_type_map[col] = 'ê¸°íƒ€'

    # ìœ í˜•ë³„ ì¤‘ìš”ë„ í•©ì‚°
    type_importance_sum = Counter()
    for _, row in df_importance.iterrows():
        feature_name = row['feature']
        importance_value = row['importance']
        type_name = feature_type_map.get(feature_name, 'ê¸°íƒ€')
        type_importance_sum[type_name] += importance_value

    for type_name, total_importance in type_importance_sum.most_common():
        count = sum(1 for f in feature_names if feature_type_map.get(f) == type_name)
        avg_importance = total_importance / count if count > 0 else 0
        print(f"  - {type_name:<8}: ì´ {total_importance:.4f} (í‰ê· : {avg_importance:.4f}, ê°œìˆ˜: {count})")


def analyze_data_quality(X_train, y_train, X_val, y_val, X_test, y_test):
    """
    ë°ì´í„° í’ˆì§ˆì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¶„ì„:")
    print(f"  - í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]:,}ê°œ ìƒ˜í”Œ, {X_train.shape[1]:,}ê°œ í”¼ì²˜")
    print(f"  - ê²€ì¦ ë°ì´í„°: {X_val.shape[0]:,}ê°œ ìƒ˜í”Œ")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ ìƒ˜í”Œ")

    # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬
    print(f"\nğŸ“ˆ íƒ€ê²Ÿ ë³€ìˆ˜(call_count) ë¶„í¬:")
    for name, y in [('í›ˆë ¨', y_train), ('ê²€ì¦', y_val), ('í…ŒìŠ¤íŠ¸', y_test)]:
        print(f"  - {name}: í‰ê· ={y.mean():.2f}, ì¤‘ê°„ê°’={y.median():.2f}, "
              f"ìµœì†Œ={y.min()}, ìµœëŒ€={y.max()}, í‘œì¤€í¸ì°¨={y.std():.2f}")

    # í”¼ì²˜ ìœ í˜• ë¶„ì„
    print(f"\nğŸ” í”¼ì²˜ ìœ í˜• ë¶„ì„:")
    feature_counts = Counter()
    for col in X_train.columns:
        if col.startswith('cat_'):
            feature_counts['ì‹ ê³ ìœ í˜•'] += 1
        elif col.startswith('subcat_'):
            feature_counts['ì„¸ë¶€ìœ í˜•'] += 1
        elif col.startswith('address_'):
            feature_counts['ì§€ì—­'] += 1
        elif any(col.startswith(x) for x in ['ta_', 'hm_', 'ws_', 'rn_']):
            feature_counts['ê¸°ìƒ'] += 1
        elif any(col.startswith(x) for x in ['year_', 'month_', 'day_', 'dayofweek_', 'quarter_', 'season_']):
            feature_counts['ì‹œê°„'] += 1
        else:
            feature_counts['ê¸°íƒ€'] += 1

    for feature_type, count in feature_counts.items():
        print(f"  - {feature_type}: {count}ê°œ")


def analyze_shap_values(model, X_data, feature_names,results_dir):
    """
    SHAPë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í•´ì„í•˜ê³  ì£¼ìš” ê·¸ë˜í”„ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("\n[+ ì¶”ê°€ ë‹¨ê³„] SHAP ë¶„ì„ ì‹œì‘...")
    try:

        explainer = shap.TreeExplainer(model)

        sample_size = min(5000, X_data.shape[0])
        sampled_X_data = shap.utils.sample(X_data, sample_size, random_state=42)  # random_state ì¶”ê°€
        print(f"  - SHAP ê°’ ê³„ì‚° ì¤‘ (ìƒ˜í”Œ í¬ê¸°: {sample_size})...")
        shap_values = explainer(sampled_X_data)
        print("  - SHAP ê°’ ê³„ì‚° ì™„ë£Œ.")

        os.makedirs(results_dir, exist_ok=True)

        # 1. SHAP ìš”ì•½ ê·¸ë˜í”„ (Summary Plot) ì €ì¥
        print("  - SHAP ìš”ì•½ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        plt.figure(figsize=(10, 8))  # ê·¸ë˜í”„ í¬ê¸° ì¡°ì •
        shap.summary_plot(shap_values, sampled_X_data, show=False, feature_names=feature_names)
        plt.tight_layout()
        summary_plot_path = os.path.join(results_dir, 'shap_summary_plot.png')  # results_dir ì‚¬ìš©
        plt.savefig(summary_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… SHAP ìš”ì•½ ê·¸ë˜í”„ ì €ì¥: {summary_plot_path}")

        # 2. SHAP ì˜ì¡´ì„± ê·¸ë˜í”„ (Dependence Plot) ì €ì¥
        print("  - SHAP ì˜ì¡´ì„± ê·¸ë˜í”„ ìƒì„± ì¤‘ (ìƒìœ„ 5ê°œ í”¼ì²˜)...")
        vals = np.abs(shap_values.values).mean(0)
        feature_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['feature', 'importance'])
        feature_importance.sort_values(by=['importance'], ascending=False, inplace=True)
        top_features = feature_importance['feature'].head(5).tolist()

        for feature in top_features:
            plt.figure(figsize=(8, 6))  # ê·¸ë˜í”„ í¬ê¸° ì¡°ì •
            # sampled_X_dataë¥¼ ì‚¬ìš©í•˜ì—¬ dependence_plot ìƒì„±
            shap.dependence_plot(feature, shap_values.values, sampled_X_data, feature_names=feature_names,
                                 interaction_index="auto", show=False)
            plt.tight_layout()
            dependence_plot_path = os.path.join(results_dir, f'shap_dependence_plot_{feature}.png')  # results_dir ì‚¬ìš©
            plt.savefig(dependence_plot_path, dpi=300, bbox_inches='tight')
            plt.close()
        print(f"âœ… SHAP ì˜ì¡´ì„± ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ.")

    except ImportError:
        print("âŒ SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install shap ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ SHAP ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback

    traceback.print_exc()


def run_stacking_model(X_train, X_val, X_test, y_train, y_val, y_test,results_dir, models_dir):
    """
    Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ Stacking ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ê³ ,
    ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ì„ í•™ìŠµ ë° í‰ê°€, ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 60)
    print("ğŸš€ Optuna ê¸°ë°˜ Stacking ì•™ìƒë¸” ìµœì í™” ì‹œì‘")
    print("=" * 60)

    # --- 1. Objective í•¨ìˆ˜ ì •ì˜ ---
    def objective(trial):
        xgb_max_depth = trial.suggest_int('xgb_max_depth', 4, 8)
        xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True)
        xgb_subsample = trial.suggest_float('xgb_subsample', 0.6, 1.0)
        xgb_colsample_bytree = trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0)
        lgbm_max_depth = trial.suggest_int('lgbm_max_depth', 4, 8)
        lgbm_learning_rate = trial.suggest_float('lgbm_learning_rate', 0.01, 0.3, log=True)

        base_models = [
            ('xgboost', xgboost.XGBRegressor(random_state=42, n_estimators=1000, n_jobs=-1, max_depth=xgb_max_depth,
                                             learning_rate=xgb_learning_rate, subsample=xgb_subsample,
                                             colsample_bytree=xgb_colsample_bytree)),
            ('lightgbm',
             LGBMRegressor(random_state=42, n_estimators=1000, n_jobs=-1, verbose=-1, max_depth=lgbm_max_depth,
                           learning_rate=lgbm_learning_rate)),
            ('huber', HuberRegressor(epsilon=1.35, max_iter=5000))
        ]
        stacking_model_trial = StackingRegressor(estimators=base_models, final_estimator=RidgeCV(), cv=3, n_jobs=-1)
        stacking_model_trial.fit(X_train, y_train)
        preds = stacking_model_trial.predict(X_val)
        rmse = np.sqrt(mean_squared_error(y_val, preds))
        return rmse

    # --- 2. Optuna Study ì‹¤í–‰ ---
    print("\n[1ë‹¨ê³„] Optuna Study ì‹¤í–‰... (n_trials ë§Œí¼ ë°˜ë³µ)")
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=50)

    print("âœ… ìµœì í™” ì™„ë£Œ!")
    print(f"  - ìµœì ì˜ ê²€ì¦ RMSE: {study.best_value:.4f}")
    print("  - ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for key, value in study.best_params.items():
        print(f"    - {key}: {value}")

    # --- 3. ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ---
    print("\n[2ë‹¨ê³„] ì°¾ì€ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ")
    best_params = study.best_params
    final_base_models = [
        ('xgboost',
         xgboost.XGBRegressor(random_state=42, n_estimators=1000, n_jobs=-1, max_depth=best_params['xgb_max_depth'],
                              learning_rate=best_params['xgb_learning_rate'], subsample=best_params['xgb_subsample'],
                              colsample_bytree=best_params['xgb_colsample_bytree'])),
        ('lightgbm', LGBMRegressor(random_state=42, n_estimators=1000, n_jobs=-1, verbose=-1,
                                   max_depth=best_params['lgbm_max_depth'],
                                   learning_rate=best_params['lgbm_learning_rate'])),
        ('huber', HuberRegressor(epsilon=1.35, max_iter=5000))
    ]
    final_meta_model = RidgeCV()
    final_stacking_model = StackingRegressor(estimators=final_base_models, final_estimator=final_meta_model, cv=5,
                                             n_jobs=-1)

    print("  - ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)")

    start_time = time.time()
    final_stacking_model.fit(X_train, y_train)
    end_time = time.time()

    print(f"âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")

    # --- 4. ìµœì¢… ëª¨ë¸ í‰ê°€ ---
    print("\n[3ë‹¨ê³„] ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    train_metrics = evaluate_model(final_stacking_model, X_train, y_train, "Optimized Stacking", "Train")
    val_metrics = evaluate_model(final_stacking_model, X_val, y_val, "Optimized Stacking", "Validation")
    test_metrics = evaluate_model(final_stacking_model, X_test, y_test, "Optimized Stacking", "Test")

    # --- 5. ìµœì¢… ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥ ---
    print("\n[4ë‹¨ê³„] ìµœì¢… ê²°ê³¼ ì‹œê°í™” ë° íŒŒì¼ ì €ì¥")
    plot_predictions(y_test, test_metrics['y_pred'], "Optimized_Stacking_Regressor", results_dir)
    xgb_in_stack = final_stacking_model.named_estimators_['xgboost']
    plot_feature_importance(xgb_in_stack, X_train.columns.tolist(), "Optimized_Stacking_XGBoost_Features", results_dir)

    try:
        print("\n[5ë‹¨ê³„] SHAP ë¶„ì„ìœ¼ë¡œ ëª¨ë¸ í•´ì„")
        xgb_in_stack = final_stacking_model.named_estimators_['xgboost']

        analyze_shap_values(xgb_in_stack, X_test, X_test.columns, results_dir)
    except Exception as e:
        print(f"âŒ SHAP ë¶„ì„ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

    print("\n[6ë‹¨ê³„] ìµœì¢… ëª¨ë¸ ë° ê²°ê³¼ ìš”ì•½ ì €ì¥")
    os.makedirs(models_dir, exist_ok=True)

    model_path = os.path.join(models_dir, 'optimized_stacking_regressor_alert.pkl')
    joblib.dump(final_stacking_model, model_path)
    print(f"âœ… ìµœì¢… Stacking ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")


    summary_path = os.path.join(results_dir, 'optimized_stacking_summary.txt')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=== Optimized Stacking ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ê²°ê³¼ ìš”ì•½ ===\n\n")
        f.write(f"Optuna Best Validation RMSE: {study.best_value:.4f}\n")
        f.write(f"í•™ìŠµ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\n")
        f.write("Best Hyperparameters:\n")
        for key, value in best_params.items():
            f.write(f"  - {key}: {value}\n")
        f.write("\n")

        meta_coefs = final_stacking_model.final_estimator_.coef_
        f.write("Meta Model (RidgeCV) Coefficients:\n")
        for i, model_info in enumerate(final_base_models):
            f.write(f"  - {model_info[0]:<10}: {meta_coefs[i]:.4f}\n")
        f.write("\n")

        for phase, metrics in [('Train', train_metrics), ('Validation', val_metrics), ('Test', test_metrics)]:
            f.write(f"--- {phase} Set Performance ---\n")
            f.write(f"  MAE : {metrics['MAE']:.4f}\n")
            f.write(f"  RMSE: {metrics['RMSE']:.4f}\n")
            f.write(f"  RÂ²  : {metrics['R2']:.4f}\n")
            f.write(f"  MAPE: {metrics['MAPE']:.2f}%\n\n")

    print(f"âœ… ëª¨ë¸ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")

    return {'model': final_stacking_model, 'metrics': test_metrics}