import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib  # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ì„ ìœ„í•´ ì¶”ê°€
import re

def load_data(call119_path, cat119_path):
    encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

    def read_csv_robust(path):
        for encoding in encodings_to_try:
            try:
                df = pd.read_csv(path, encoding=encoding, index_col=False)
                print(f"âœ… '{os.path.basename(path)}' íŒŒì¼ì„ '{encoding}' ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì„±ê³µ.")
                return df
            except (UnicodeDecodeError, pd.errors.ParserError):
                continue
        raise Exception(f"âŒ '{os.path.basename(path)}' íŒŒì¼ì„ ë‹¤ìŒ ì¸ì½”ë”©ìœ¼ë¡œ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {encodings_to_try}")

    print(f"\n--- ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹œì‘ ---")
    call119_df = read_csv_robust(call119_path)
    cat119_df = read_csv_robust(cat119_path)

    print(f"call119 ë°ì´í„° í¬ê¸°: {call119_df.shape}")
    print(f"cat119 ë°ì´í„° í¬ê¸°: {cat119_df.shape}")

    return call119_df, cat119_df

def preprocess_people_data(people_raw_path):
    print("ì¸êµ¬ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

    # ë©€í‹°í—¤ë” ì½ê¸°
    df_raw = pd.read_csv(people_raw_path, header=[0, 1])

    # ë©€í‹°ì¸ë±ìŠ¤ -> ë‹¨ì¼ ì»¬ëŸ¼ìœ¼ë¡œ flatten
    df_raw.columns = [
        f"{upper}_{lower}".strip() if lower != '' and lower != 'Unnamed: 1_level_1' else upper
        for upper, lower in df_raw.columns
    ]

    # ë°ì´í„° íŒŒíŠ¸ë§Œ ì¶”ì¶œ (index ì´ˆê¸°í™”)
    df_raw.reset_index(drop=True, inplace=True)

    # í•©ê³„ / ë‚¨ / ì—¬ ìˆœì„œëŒ€ë¡œ ì¡´ì¬í•œë‹¤ê³  ê°€ì •
    total_row = df_raw.iloc[0]
    male_row = df_raw.iloc[1]
    female_row = df_raw.iloc[2]

    people_data = []
    for col in df_raw.columns[2:]:
        gu, dong = col.split('_')
        total = int(total_row[col])
        male = int(male_row[col])
        female = int(female_row[col])
        people_data.append([gu, dong, total, male, female])

    df_people = pd.DataFrame(people_data, columns=[
        'address_gu', 'sub_address', 'population_total', 'population_male', 'population_female'
    ])

    print(f"âœ… ì¸êµ¬ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {df_people.shape}")
    if 'ê°•ì„œêµ¬' in df_people['address_gu'].unique():
        print("    -> 'ê°•ì„œêµ¬' ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("    -> âš ï¸ ê²½ê³ : ì²˜ë¦¬ í›„ì—ë„ 'ê°•ì„œêµ¬' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ CSV íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    return df_people

def create_dong_aggregation_mapping(people_df):
    """
    ë™ ì´ë¦„ íŒ¨í„´ì„ ë¶„ì„í•´ì„œ ìë™ìœ¼ë¡œ í†µí•© ë§¤í•‘ì„ ìƒì„±
    """
    dong_mapping = {}

    # 1. ê¸°ë³¸ ë™ ì´ë¦„ ì¶”ì¶œ (ìˆ«ì ì œê±°)
    people_df_copy = people_df.copy()
    people_df_copy['base_dong'] = people_df_copy['sub_address'].str.replace(r'\d+ë™$', 'ë™', regex=True)

    # 2. ê° ê¸°ë³¸ ë™ë³„ë¡œ ê·¸ë£¹í™”
    base_dong_groups = people_df_copy.groupby(['address_gu', 'base_dong'])

    for (gu, base_dong), group in base_dong_groups:
        if len(group) > 1:
            # í•©ê³„ ê³„ì‚°
            total_pop = group['population_total'].sum()
            male_pop = group['population_male'].sum()
            female_pop = group['population_female'].sum()

            # í†µí•© ë™ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
            dong_mapping[f"{gu}_{base_dong}"] = {
                'population_total': total_pop,
                'population_male': male_pop,
                'population_female': female_pop,
                'source_dongs': group['sub_address'].tolist()
            }

            print(f"í†µí•© ë™ ìƒì„±: {gu} {base_dong} = {group['sub_address'].tolist()}")

    return dong_mapping


def merge_people_data(merged_df, people_df):

    print("í–¥ìƒëœ ì¸êµ¬ ë°ì´í„° ë³‘í•© ì¤‘...")

    # 1. ë™ í†µí•© ë§¤í•‘ ìƒì„±
    dong_mapping = create_dong_aggregation_mapping(people_df)

    # 2. ë³‘í•©í‚¤ ì „ì²˜ë¦¬ (ì†Œë¬¸ì, ê³µë°± ì œê±°)
    merged_df_copy = merged_df.copy()
    people_df_copy = people_df.copy()

    for col in ['address_gu', 'sub_address']:
        merged_df_copy[col] = merged_df_copy[col].astype(str).str.strip().str.lower()
        people_df_copy[col] = people_df_copy[col].astype(str).str.strip().str.lower()

    # 3. ê¸°ë³¸ ë³‘í•© ì‹œë„ (ì •í™• ë§¤ì¹­ - ëŒ€ì €1ë™ = ëŒ€ì €1ë™)
    merged_result = pd.merge(
        merged_df_copy,
        people_df_copy[['address_gu', 'sub_address', 'population_total', 'population_male', 'population_female']],
        on=['address_gu', 'sub_address'],
        how='left'
    )

    # 4. ë§¤ì¹­ ì•ˆ ëœ í–‰ë“¤ì— ëŒ€í•´ í†µí•© ë™ ë§¤í•‘ ì ìš©
    unmatched_mask = merged_result['population_total'].isna()

    print(f"ì§ì ‘ ë§¤ì¹­ ì„±ê³µ: {(~unmatched_mask).sum()}ê°œ")
    print(f"ì§ì ‘ ë§¤ì¹­ ì‹¤íŒ¨: {unmatched_mask.sum()}ê°œ")

    for idx in merged_result[unmatched_mask].index:
        gu = merged_result.loc[idx, 'address_gu']
        dong = merged_result.loc[idx, 'sub_address']

        # ê¸°ë³¸ ë™ ì´ë¦„ ìƒì„± (ìˆ«ì ì œê±°) - ë™ëŒ€ì‹ ë™ <- ë™ëŒ€ì‹ 1ë™+ë™ëŒ€ì‹ 2ë™+ë™ëŒ€ì‹ 3ë™
        base_dong = re.sub(r'\d+ë™$', 'ë™', dong)
        mapping_key = f"{gu}_{base_dong}"

        if mapping_key in dong_mapping:
            # í†µí•© ë™ ë°ì´í„° ì‚¬ìš©
            merged_result.loc[idx, 'population_total'] = dong_mapping[mapping_key]['population_total']
            merged_result.loc[idx, 'population_male'] = dong_mapping[mapping_key]['population_male']
            merged_result.loc[idx, 'population_female'] = dong_mapping[mapping_key]['population_female']

            source_dongs = dong_mapping[mapping_key]['source_dongs']
          #  print(f"í†µí•© ë™ ë§¤í•‘ ì ìš©: {gu} {dong} <- {source_dongs}")

    # 5. ì—¬ì „íˆ ë§¤ì¹­ ì•ˆ ëœ ê²½ìš° êµ¬ë³„ ìµœì†Œê°’ ì‚¬ìš©
    still_unmatched = merged_result['population_total'].isna()

    if still_unmatched.sum() > 0:
        print(f"êµ¬ë³„ ìµœì†Œê°’ìœ¼ë¡œ ì²˜ë¦¬í•  ë™: {still_unmatched.sum()}ê°œ")

        # êµ¬ë³„ ìµœì†Œê°’ ê³„ì‚° - ë” ì•ˆì •ì ì¸ ë°©ë²• ì‚¬ìš©
        gu_min_stats = people_df_copy.groupby('address_gu').agg({
            'population_total': 'min',
            'population_male': 'min',
            'population_female': 'min'
        })

        for idx in merged_result[still_unmatched].index:
            gu = merged_result.loc[idx, 'address_gu']
            dong = merged_result.loc[idx, 'sub_address']

            if gu in gu_min_stats.index:
                merged_result.loc[idx, 'population_total'] = gu_min_stats.loc[gu, 'population_total']
                merged_result.loc[idx, 'population_male'] = gu_min_stats.loc[gu, 'population_male']
                merged_result.loc[idx, 'population_female'] = gu_min_stats.loc[gu, 'population_female']

                print(f"êµ¬ë³„ ìµœì†Œê°’ ì ìš©: {gu} {dong} -> {gu_min_stats.loc[gu, 'population_total']}ëª…")
            else:
                # êµ¬ ì •ë³´ë„ ì—†ëŠ” ê²½ìš° ì „ì²´ ìµœì†Œê°’ ì‚¬ìš©
                min_total = people_df_copy['population_total'].min()
                min_male = people_df_copy['population_male'].min()
                min_female = people_df_copy['population_female'].min()

                merged_result.loc[idx, 'population_total'] = min_total
                merged_result.loc[idx, 'population_male'] = min_male
                merged_result.loc[idx, 'population_female'] = min_female

                print(f"ì „ì²´ ìµœì†Œê°’ ì ìš©: {gu} {dong} -> {min_total}ëª…")

    # 6. ìµœì¢… ê²°ê³¼ í™•ì¸
    final_unmatched = merged_result['population_total'].isna().sum()
    if final_unmatched > 0:
        print(f"âš ï¸  ìµœì¢…ì ìœ¼ë¡œ ë§¤ì¹­ë˜ì§€ ì•Šì€ í–‰: {final_unmatched}ê°œ")
    else:
        print("âœ… ëª¨ë“  í–‰ì˜ ì¸êµ¬ ë°ì´í„° ë§¤ì¹­ ì™„ë£Œ")

    # 7. ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ì™€ êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ ì¸êµ¬ ë°ì´í„°ë§Œ ì¶”ê°€
    merged_df_result = merged_df.copy()
    merged_df_result['population_total'] = merged_result['population_total']
    merged_df_result['population_male'] = merged_result['population_male']
    merged_df_result['population_female'] = merged_result['population_female']

    print(f"âœ… í–¥ìƒëœ ì¸êµ¬ ë°ì´í„° ë³‘í•© ì™„ë£Œ: {merged_df_result.shape}")

    return merged_df_result


def preprocess_traffic_data(traffic_raw_path, year):
    """
    êµí†µì‚¬ê³  í†µê³„ ì›ë³¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³ , ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print(f"êµí†µì‚¬ê³  ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ({year}ë…„)...")

    try:
        # ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ë¡œë“œ ì‹œë„
        encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']
        df_raw = None
        for encoding in encodings_to_try:
            try:
                df_raw = pd.read_csv(traffic_raw_path, encoding=encoding)
                print(f"âœ… '{os.path.basename(traffic_raw_path)}' íŒŒì¼ ë¡œë“œ ì„±ê³µ (ì¸ì½”ë”©: {encoding})")
                break
            except (UnicodeDecodeError, pd.errors.ParserError):
                continue
        if df_raw is None:
            raise Exception("ëª¨ë“  ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: êµí†µì‚¬ê³  ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {traffic_raw_path}")
        return pd.DataFrame()

    metrics_map = {
        'ì‚¬ê³ [ê±´]': 'accident_count',
        'ì‚¬ë§[ëª…]': 'fatality_count',
        'ë¶€ìƒ[ëª…]': 'injury_count',
        '(ì¤‘ìƒì[ëª…])': 'serious_injury_count'
    }

    processed_dfs = []
    day_cols = [f'{i:02d}ì¼' for i in range(1, 32)]

    for metric_kr, metric_en in metrics_map.items():
        if 'ì‚¬ê³ ì¼' not in df_raw.columns:
            print(f"âŒ ì˜¤ë¥˜: '{os.path.basename(traffic_raw_path)}' íŒŒì¼ì— 'ì‚¬ê³ ì¼' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        df_metric = df_raw[df_raw['ì‚¬ê³ ì¼'] == metric_kr].copy()
        df_melted = df_metric.melt(id_vars=['ì‹œêµ°êµ¬', 'ì‚¬ê³ ì›”'], value_vars=day_cols, var_name='ì¼', value_name=metric_en)
        df_melted[metric_en] = pd.to_numeric(df_melted[metric_en].replace('-', '0'), errors='coerce').fillna(0).astype(
            int)
        processed_dfs.append(df_melted)

    from functools import reduce
    df_merged = reduce(lambda left, right: pd.merge(left, right, on=['ì‹œêµ°êµ¬', 'ì‚¬ê³ ì›”', 'ì¼']), processed_dfs)

    df_merged['ì‚¬ê³ ì›”'] = df_merged['ì‚¬ê³ ì›”'].str.replace('ì›”', '').astype(int)
    df_merged['ì¼'] = df_merged['ì¼'].str.replace('ì¼', '').astype(int)

    date_str = str(year) + '-' + df_merged['ì‚¬ê³ ì›”'].astype(str) + '-' + df_merged['ì¼'].astype(str)
    df_merged['tm_dt'] = pd.to_datetime(date_str, errors='coerce')

    df_merged.dropna(subset=['tm_dt'], inplace=True)
    df_merged['tm'] = df_merged['tm_dt'].dt.strftime('%Y%m%d')

    df_merged.rename(columns={'ì‹œêµ°êµ¬': 'address_gu'}, inplace=True)
    df_merged['total_casualties'] = df_merged['fatality_count'] + df_merged['injury_count']

    final_features = [
        'tm', 'address_gu',
        'accident_count',
        'total_casualties'
    ]
    df_final = df_merged[final_features]

    print(f"âœ… {year}ë…„ êµí†µì‚¬ê³  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (ê¸°ë³¸ í”¼ì²˜ë§Œ ì‚¬ìš©): {df_final.shape}")
    return df_final

def preprocess_weather_alerts(df):
    """
    ê¸°ìƒ íŠ¹ë³´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë‚ ì§œë³„ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.
    """
    print("ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

    if df.empty:
        print("âŒ ê¸°ìƒ íŠ¹ë³´ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    # ì»¬ëŸ¼ ì´ë¦„ í›„ë³´ í™•ì¥ (ë” ë‹¤ì–‘í•œ ê²½ìš°ì˜ ìˆ˜ ê³ ë ¤)
    possible_cols = [
        'warning_type', 'íŠ¹ë³´ì¢…ë¥˜', 'alert_type', 'íŠ¹ë³´ë‚´ìš©',
        'warning', 'alert', 'type', 'ì¢…ë¥˜', 'ë‚´ìš©'
    ]

    alert_col_name = None
    for col in possible_cols:
        if col in df.columns:
            alert_col_name = col
            print(f"âœ… ê¸°ìƒ íŠ¹ë³´ ì»¬ëŸ¼ìœ¼ë¡œ '{alert_col_name}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            break

    if alert_col_name is None:
        print(f"âŒ ê¸°ìƒ íŠ¹ë³´ ìœ í˜• ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ì‹¤ì œ ì»¬ëŸ¼: {list(df.columns)}")
        return pd.DataFrame()

    try:
        # 1. ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ (ë” ìœ ì—°í•˜ê²Œ)
        df = df.copy()
        df['tm'] = pd.to_datetime(df['tm'], errors='coerce')

        # ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ ì œê±°
        invalid_dates = df['tm'].isna().sum()
        if invalid_dates > 0:
            print(f"âš ï¸  ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ {invalid_dates}ê°œ ì œê±°")
            df = df.dropna(subset=['tm'])

        if df.empty:
            print("âŒ ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        # 2. íŠ¹ë³´ ë°ì´í„° ì •ë¦¬ ë° ë¶„ë¦¬
        # NaN ê°’ ì²˜ë¦¬
        df[alert_col_name] = df[alert_col_name].fillna('')

        # ë¬¸ìì—´ ì •ë¦¬: ì£¼ì˜ë³´/ê²½ë³´ ì œê±°, ê³µë°± ì •ë¦¬
        df['cleaned_alerts'] = (df[alert_col_name]
                                .str.replace('ì£¼ì˜ë³´|ê²½ë³´', '', regex=True)
                                .str.replace(r'\s+', ' ', regex=True)  # ì—°ì†ëœ ê³µë°± ì œê±°
                                .str.strip())

        # 3. ì‰¼í‘œë¡œ ë¶„ë¦¬ëœ íŠ¹ë³´ë“¤ì„ ê°œë³„ í–‰ìœ¼ë¡œ ë¶„í• 
        df_expanded = df.assign(
            alert_type=df['cleaned_alerts'].str.split(',')
        ).explode('alert_type')

        # ë¹ˆ ê°’ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” íŠ¹ë³´ëª… ì œê±°
        df_expanded = df_expanded[
            df_expanded['alert_type'].str.strip().str.len() > 0
            ]
        df_expanded['alert_type'] = df_expanded['alert_type'].str.strip()

        # 4. ì›-í•« ì¸ì½”ë”©
        alert_dummies = pd.get_dummies(df_expanded['alert_type'], prefix='alert')

        # 5. ë‚ ì§œì™€ ê²°í•©í•˜ì—¬ ì§‘ê³„
        alert_processed = pd.concat([
            df_expanded[['tm']].reset_index(drop=True),
            alert_dummies.reset_index(drop=True)
        ], axis=1)

        # 6. ë‚ ì§œë³„ë¡œ ì§‘ê³„ (íŠ¹ë³´ê°€ í•˜ë£¨ì— ì—¬ëŸ¬ ë²ˆ ë‚˜ì™€ë„ 1ë¡œ ì²˜ë¦¬)
        alert_agg = alert_processed.groupby('tm').max().reset_index()

        # 7. ê²°ê³¼ ê²€ì¦
        alert_features = [col for col in alert_agg.columns if col.startswith('alert_')]
        print(f"âœ… ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   - ì²˜ë¦¬ëœ ë‚ ì§œ ë²”ìœ„: {alert_agg['tm'].min()} ~ {alert_agg['tm'].max()}")
        print(f"   - ìƒì„±ëœ íŠ¹ë³´ í”¼ì²˜: {len(alert_features)}ê°œ")
        print(f"   - íŠ¹ë³´ ì¢…ë¥˜: {[col.replace('alert_', '') for col in alert_features]}")

        return alert_agg

    except Exception as e:
        print(f"âŒ ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()


def merge_weather_alerts(merged_df, alert_agg_df):
    """
    ê¸°ìƒ íŠ¹ë³´ ë°ì´í„°ë¥¼ ë©”ì¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©í•˜ëŠ” ë³„ë„ í•¨ìˆ˜
    """
    print("ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ë³‘í•© ì¤‘...")

    if alert_agg_df.empty:
        print("âš ï¸  ê¸°ìƒ íŠ¹ë³´ ë°ì´í„°ê°€ ì—†ì–´ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return merged_df

    # ë‚ ì§œ ì»¬ëŸ¼ íƒ€ì… í†µì¼
    merged_df['tm'] = pd.to_datetime(merged_df['tm'], format='%Y%m%d', errors='coerce')
    alert_agg_df['tm'] = pd.to_datetime(alert_agg_df['tm'], errors='coerce')

    # ë³‘í•© ì „ ë°ì´í„° í™•ì¸
    print(f"   - ë©”ì¸ ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {merged_df['tm'].min()} ~ {merged_df['tm'].max()}")
    print(f"   - íŠ¹ë³´ ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {alert_agg_df['tm'].min()} ~ {alert_agg_df['tm'].max()}")

    # Left Joinìœ¼ë¡œ ë³‘í•©
    result_df = pd.merge(merged_df, alert_agg_df, on='tm', how='left')

    # íŠ¹ë³´ ì»¬ëŸ¼ë“¤ì˜ NaNì„ 0ìœ¼ë¡œ ì±„ì›€
    alert_cols = [col for col in result_df.columns if col.startswith('alert_')]
    result_df[alert_cols] = result_df[alert_cols].fillna(0).astype(int)

    # ë³‘í•© ê²°ê³¼ í™•ì¸
    total_alerts = result_df[alert_cols].sum().sum()
    coverage = (result_df[alert_cols].sum(axis=1) > 0).mean()

    print(f"âœ… ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ë³‘í•© ì™„ë£Œ:")
    print(f"   - íŠ¹ë³´ í”¼ì²˜ ìˆ˜: {len(alert_cols)}ê°œ")
    print(f"   - ì „ì²´ íŠ¹ë³´ ë°œìƒ íšŸìˆ˜: {total_alerts}íšŒ")
    print(f"   - íŠ¹ë³´ê°€ ìˆëŠ” ë‚ ì˜ ë¹„ìœ¨: {coverage:.1%}")

    return result_df


def clean_column_names(df):
    """ì»¬ëŸ¼ëª… ì •ë¦¬ í•¨ìˆ˜"""
    new_columns = []
    for col in df.columns:
        col_clean = col.lower()
        if '.' in col_clean:
            col_clean = col_clean.split('.')[-1]
        new_columns.append(col_clean)
    df.columns = new_columns
    return df


def aggregate_cat119_by_location_time(cat119_df):
    """cat119 ë°ì´í„°ë¥¼ ì§€ì—­ë³„, ì‹œê°„ë³„ë¡œ ì§‘ê³„"""
    print("cat119 ë°ì´í„° ì§‘ê³„ ì¤‘...")
    cat_dummies = pd.get_dummies(cat119_df['cat'], prefix='cat')
    subcat_dummies = pd.get_dummies(cat119_df['sub_cat'], prefix='subcat')
    cat119_expanded = pd.concat([
        cat119_df[['tm', 'address_city', 'address_gu', 'sub_address', 'stn']],
        cat_dummies,
        subcat_dummies,
        cat119_df[['call_count']]
    ], axis=1)
    groupby_cols = ['tm', 'address_city', 'address_gu', 'sub_address', 'stn']
    numeric_cols = cat_dummies.columns.tolist() + subcat_dummies.columns.tolist() + ['call_count']
    agg_dict = {col: 'sum' for col in numeric_cols}
    cat119_agg = cat119_expanded.groupby(groupby_cols).agg(agg_dict).reset_index()
    print(f"cat119 ì§‘ê³„ ì™„ë£Œ: {cat119_agg.shape}")
    print(f"ìƒì„±ëœ ì‹ ê³  ìœ í˜• í”¼ì²˜ ìˆ˜: {len(cat_dummies.columns) + len(subcat_dummies.columns)}")
    return cat119_agg


def merge_call_cat_data(call119_df, cat119_agg):
    """call119ì™€ ì§‘ê³„ëœ cat119 ë°ì´í„° ë³‘í•©"""
    print("call119ì™€ cat119 ë°ì´í„° ë³‘í•© ì¤‘...")
    merge_cols = ['tm', 'address_city', 'address_gu', 'sub_address', 'stn']
    for col in merge_cols:
        if col in call119_df.columns and col in cat119_agg.columns:
            if call119_df[col].dtype == 'object':
                call119_df[col] = call119_df[col].astype(str).str.strip()
                cat119_agg[col] = cat119_agg[col].astype(str).str.strip()
    merged_df = call119_df.merge(cat119_agg, on=merge_cols, how='left', suffixes=('', '_cat'))
    cat_cols = [col for col in cat119_agg.columns if col.startswith(('cat_', 'subcat_'))]
    merged_df[cat_cols] = merged_df[cat_cols].fillna(0)
    if 'call_count_cat' in merged_df.columns:
        merged_df['call_count'] = merged_df['call_count_cat'].fillna(merged_df['call_count'])
        merged_df.drop(columns=['call_count_cat'], inplace=True)
    print(f"ë³‘í•© ì™„ë£Œ: {merged_df.shape}")
    print(f"ë³‘í•© ì„±ê³µë¥ : {(merged_df[cat_cols].sum(axis=1) > 0).mean():.2%}")
    return merged_df


def handle_missing_values(df):
    """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
    print("ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
    weather_cols = ['ta_max', 'ta_min', 'ta_max_min', 'hm_min', 'hm_max', 'ws_max', 'ws_ins_max', 'rn_day']
    for col in weather_cols:
        if col in df.columns:
            df[col] = df[col].replace(-99.0, np.nan)
    missing_counts = df.isnull().sum()
    if missing_counts.sum() > 0:
        print("ê²°ì¸¡ì¹˜ í˜„í™©:")
        for col, count in missing_counts[missing_counts > 0].items():
            print(f"  {col}: {count}ê°œ ({count / len(df) * 100:.1f}%)")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df[col].isnull().sum() > 0:
                if 'address_gu' in df.columns:
                    df[col] = df.groupby('address_gu')[col].transform(lambda x: x.fillna(x.mean()))
                df[col] = df[col].fillna(df[col].mean())
    return df


def create_time_features(df, time_col='tm'):
    """ì‹œê°„ ê´€ë ¨ í”¼ì²˜ ìƒì„±"""
    print("ì‹œê°„ ê´€ë ¨ í”¼ì²˜ ìƒì„± ì¤‘...")
    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
    df['year'] = df[time_col].dt.year
    df['month'] = df[time_col].dt.month
    df['day'] = df[time_col].dt.day
    df['dayofweek'] = df[time_col].dt.dayofweek
    df['quarter'] = df[time_col].dt.quarter
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    df['season'] = df['month'].map({
        12: 'winter', 1: 'winter', 2: 'winter',
        3: 'spring', 4: 'spring', 5: 'spring',
        6: 'summer', 7: 'summer', 8: 'summer',
        9: 'autumn', 10: 'autumn', 11: 'autumn'
    })
    df['is_holiday'] = 0
    print("ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ")
    return df


def preprocess_data(call119_df, cat119_df, weather_alert_df, traffic_df,people_df=None):
    """ê°œì„ ëœ ë©”ì¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    print("\n--- ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ---")

    # 1. ì»¬ëŸ¼ëª… ì •ë¦¬
    call119_df = clean_column_names(call119_df.copy())
    cat119_df = clean_column_names(cat119_df.copy())
    weather_alert_df = clean_column_names(weather_alert_df.copy())

    # 2. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
    for df in [call119_df, cat119_df]:
        if 'unnamed: 0' in df.columns:
            df.drop(columns=['unnamed: 0'], inplace=True)

    # 3. call119ì™€ cat119 ë³‘í•©
    if not cat119_df.empty:
        cat119_agg = aggregate_cat119_by_location_time(cat119_df)
        merged_df = merge_call_cat_data(call119_df, cat119_agg)
    else:
        merged_df = call119_df.copy()

    # 4. ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•©
    if not weather_alert_df.empty:
        alert_agg_df = preprocess_weather_alerts(weather_alert_df)
        merged_df = merge_weather_alerts(merged_df, alert_agg_df)
    else:
        print("âš ï¸  ê¸°ìƒ íŠ¹ë³´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        # 5. êµí†µì‚¬ê³  ë°ì´í„° ë³‘í•©
    if traffic_df is not None and not traffic_df.empty:
        print("êµí†µì‚¬ê³  ë°ì´í„° ë³‘í•© ì¤‘...")
        # tm ì»¬ëŸ¼ íƒ€ì…ì„ ë³‘í•© ì „ì— í†µì¼ (YYYYMMDD í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ)
        merged_df['tm'] = merged_df['tm'].astype(str).str.replace('-', '')
        traffic_df['tm'] = traffic_df['tm'].astype(str)

        # 'address_gu'ì˜ ë°ì´í„° íƒ€ì…ê³¼ ê³µë°± ë“±ì„ í†µì¼
        merged_df['address_gu'] = merged_df['address_gu'].astype(str).str.strip()
        traffic_df['address_gu'] = traffic_df['address_gu'].astype(str).str.strip()

        merged_df = pd.merge(merged_df, traffic_df, on=['tm', 'address_gu'], how='left')

        # ë³‘í•© í›„ ìƒì„±ëœ NaNì€ 0ìœ¼ë¡œ ì±„ì›€ (ì‚¬ê³ ê°€ ì—†ì—ˆë˜ ë‚ )
        traffic_cols = ['accident_count', 'total_casualties', 'injury_per_accident', 'accident_moving_average_3days']
        for col in traffic_cols:
            if col in merged_df.columns:
                merged_df[col] = merged_df[col].fillna(0)
        print(f"âœ… êµí†µì‚¬ê³  ë°ì´í„° ë³‘í•© ì™„ë£Œ. í˜„ì¬ ë°ì´í„° í¬ê¸°: {merged_df.shape}")
    else:
        print("âš ï¸  êµí†µì‚¬ê³  ë°ì´í„°ê°€ ì—†ì–´ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    import sys
    print("\n[ë””ë²„ê¹…] êµí†µì‚¬ê³  ë°ì´í„° ë³‘í•© í›„ 'address_gu' ëª©ë¡ í™•ì¸")



    if people_df is not None:
        merged_df = merge_people_data(merged_df, people_df)


    # 5. ë‚˜ë¨¸ì§€ ì „ì²˜ë¦¬ ê³¼ì •
    merged_df = handle_missing_values(merged_df)
    merged_df = create_time_features(merged_df, 'tm')

    print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.")
    print(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {merged_df.shape}")

    return merged_df


def encode_features(df):

    print("í”¼ì²˜ ì¸ì½”ë”© ì‹œì‘...")
    final_df = df.copy()
    categorical_cols = [
        'address_city', 'address_gu', 'sub_address',
        'year', 'month', 'day', 'dayofweek', 'quarter', 'season'
    ]
    existing_categorical_cols = [col for col in categorical_cols if col in final_df.columns]

    if existing_categorical_cols:

        final_df = pd.get_dummies(final_df, columns=existing_categorical_cols, drop_first=False)
        print(f"One-hot ì¸ì½”ë”© ì™„ë£Œ: {len(existing_categorical_cols)}ê°œ ë²”ì£¼í˜• ë³€ìˆ˜")

    cat_features = [col for col in final_df.columns if col.startswith(('cat_', 'subcat_'))]
    print(f"ì‹ ê³  ìœ í˜• í”¼ì²˜: {len(cat_features)}ê°œ")
    print("í”¼ì²˜ ì¸ì½”ë”© ì™„ë£Œ.")
    print(f"ìµœì¢… í”¼ì²˜ ìˆ˜: {final_df.shape[1]}")
    return final_df


def split_data_time_series(df, target_column='call_count', train_ratio=0.8, val_ratio=0.1):
    """ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§"""
    print("ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§ ì‹œì‘...")
    time_col = 'tm'
    df_sorted = df.sort_values(by=time_col).reset_index(drop=True)

    n = len(df_sorted)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))
    train_df = df_sorted.iloc[:train_end]
    val_df = df_sorted.iloc[train_end:val_end]
    test_df = df_sorted.iloc[val_end:]

    cols_to_drop = [target_column, 'ID', 'tm'] + [col for col in df.columns if 'datetime' in str(df[col].dtype)]
    X_train = train_df.drop(columns=cols_to_drop, errors='ignore')
    y_train = train_df[target_column]
    X_val = val_df.drop(columns=cols_to_drop, errors='ignore')
    y_val = val_df[target_column]
    X_test = test_df.drop(columns=cols_to_drop, errors='ignore')
    y_test = test_df[target_column]

    numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()
    scaler = StandardScaler()
    if numerical_cols:
        # train ë°ì´í„°ì— ë§ì¶° ìŠ¤ì¼€ì¼ë§í•˜ê³ , val/test ë°ì´í„°ëŠ” ê·¸ê²ƒìœ¼ë¡œ ë³€í™˜ë§Œ ìˆ˜í–‰
        X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
        X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])
        X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])
        print(f"ìŠ¤ì¼€ì¼ë§ ì ìš© í”¼ì²˜: {len(numerical_cols)}ê°œ")

    print("ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ:")
    print(f"  - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
    print(f"  - ì „ì²´ í”¼ì²˜ ìˆ˜: {X_train.shape[1]}")
    return X_train, X_val, X_test, y_train, y_val, y_test, scaler


def save_processed_data(output_dir, X_train, X_val, X_test, y_train, y_val, y_test, scaler):
    """ì „ì²˜ë¦¬ëœ ë°ì´í„°ì™€ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    print(f"\n--- ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì‹œì‘ ---")
    os.makedirs(output_dir, exist_ok=True)
    print(f"ì €ì¥ ê²½ë¡œ: '{output_dir}'")

    try:
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë‹¤ì‹œ í•©ì³ì„œ ì €ì¥ (Xì™€ yë¥¼ í•©ì³ì•¼ í™•ì¸í•˜ê¸° ìš©ì´)
        train_data = pd.concat([X_train, y_train], axis=1)
        val_data = pd.concat([X_val, y_val], axis=1)
        test_data = pd.concat([X_test, y_test], axis=1)

        # CSV íŒŒì¼ë¡œ ì €ì¥ (utf-8-sigëŠ” Excelì—ì„œ í•œê¸€ ê¹¨ì§ ë°©ì§€)
        train_data.to_csv(os.path.join(output_dir, 'train_data.csv'), index=False, encoding='utf-8-sig')
        val_data.to_csv(os.path.join(output_dir, 'validation_data.csv'), index=False, encoding='utf-8-sig')
        test_data.to_csv(os.path.join(output_dir, 'test_data.csv'), index=False, encoding='utf-8-sig')
        print("âœ… Train/Validation/Test ë°ì´í„°ì…‹ CSV ì €ì¥ ì™„ë£Œ.")

        # ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ ì €ì¥
        joblib.dump(scaler, os.path.join(output_dir, 'scaler.joblib'))
        print("âœ… StandardScaler ê°ì²´ ì €ì¥ ì™„ë£Œ.")

    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print("--- ë°ì´í„° ì €ì¥ ì™„ë£Œ ---")


def load_weather_alert_data(alert_path):
    """ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜"""
    encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(alert_path, encoding=encoding, index_col=False)
            print(f"âœ… '{os.path.basename(alert_path)}' íŒŒì¼ì„ '{encoding}' ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì„±ê³µ.")
            return df
        except (UnicodeDecodeError, pd.errors.ParserError):
            continue

    print(f"âŒ '{os.path.basename(alert_path)}' íŒŒì¼ì„ ë‹¤ìŒ ì¸ì½”ë”©ìœ¼ë¡œ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {encodings_to_try}")
    return pd.DataFrame()


# ==============================================================================
# ===== ë©”ì¸ ì‹¤í–‰ ë¸”ë¡: ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ ì•„ë˜ ì½”ë“œê°€ ë™ì‘í•©ë‹ˆë‹¤ =====
# ==============================================================================
if __name__ == '__main__':

    # --- ì„¤ì • ---
    BASE_DIR = os.path.dirname(__file__)
    # ê²½ë¡œë¥¼ ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.
    # src í´ë” ì•ˆì— preprocess.pyê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    DATA_DIR = os.path.join(BASE_DIR, '..', 'data')

    CALL119_TRAIN_PATH = os.path.join(DATA_DIR, 'call119_train1.csv')
    CAT119_TRAIN_PATH = os.path.join(DATA_DIR, 'cat119_train1.csv')
    ALERT_TRAIN_PATH = os.path.join(DATA_DIR, 'weather_alert.csv')
    PEOPLE_RAW_PATH = os.path.join(DATA_DIR, '2023_people.csv')
    OUTPUT_DIRECTORY = './preprocessed_data'

    TRAFFIC_FILES_INFO = [
        {'path': os.path.join(DATA_DIR, 'Report_2020.csv'), 'year': 2020},
        {'path': os.path.join(DATA_DIR, 'Report_2021.csv'), 'year': 2021},
        {'path': os.path.join(DATA_DIR, 'Report_2022.csv'), 'year': 2022},
        {'path': os.path.join(DATA_DIR, 'Report_2023.csv'), 'year': 2023},
    ]

    # === ì‹¤í–‰ ===
    # 1. ë°ì´í„° ë¡œë“œ
    print("--- ë°ì´í„° ë¡œë“œ ì‹œì‘ ---")
    call_df, cat_df = load_data(CALL119_TRAIN_PATH, CAT119_TRAIN_PATH)

    # ê¸°ìƒ íŠ¹ë³´ ë°ì´í„° ë¡œë“œ
    alert_df = load_weather_alert_data(ALERT_TRAIN_PATH)
    people_df = preprocess_people_data(PEOPLE_RAW_PATH)

    all_traffic_dfs = []
    for file_info in TRAFFIC_FILES_INFO:
        if os.path.exists(file_info['path']):
            df_year = preprocess_traffic_data(file_info['path'], file_info['year'])
            all_traffic_dfs.append(df_year)
        else:
            print(f"âš ï¸ ê²½ê³ : '{file_info['path']}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")

    if all_traffic_dfs:
        traffic_df = pd.concat(all_traffic_dfs, ignore_index=True)
        print(f"\nâœ… ëª¨ë“  ì—°ë„ êµí†µì‚¬ê³  ë°ì´í„° í†µí•© ì™„ë£Œ. ì „ì²´ í¬ê¸°: {traffic_df.shape}")
    else:
        traffic_df = pd.DataFrame()  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì´ˆê¸°í™”
        print("\nâš ï¸ ì²˜ë¦¬í•  êµí†µì‚¬ê³  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    print("\n--- ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ---")
    preprocessed_df = preprocess_data(call_df, cat_df, alert_df, traffic_df, people_df)

    # 3. í”¼ì²˜ ì¸ì½”ë”©
    print("\n--- í”¼ì²˜ ì¸ì½”ë”© ì‹œì‘ ---")
    encoded_df = encode_features(preprocessed_df)

    # 4. ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœì˜ ë°ì´í„° ì €ì¥
    print(f"\n--- ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœì˜ ë°ì´í„° ì €ì¥ ì‹œì‘ ---")
    try:
        os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)
        human_readable_path = os.path.join(OUTPUT_DIRECTORY, 'preprocessed_human_readable_traffic.csv')
        encoded_df.to_csv(human_readable_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ìŠ¤ì¼€ì¼ë§ ì „ ë°ì´í„° ì €ì¥ ì™„ë£Œ: '{human_readable_path}'")
    except Exception as e:
        print(f"âŒ ì½ê¸° ì‰¬ìš´ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 5. ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
    print("\n--- ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§ ì‹œì‘ ---")
    X_train, X_val, X_test, y_train, y_val, y_test, scaler_obj = split_data_time_series(
        encoded_df,
        target_column='call_count'
    )

    # 6. ìµœì¢… ê²°ê³¼ë¬¼(ëª¨ë¸ í•™ìŠµìš©) íŒŒì¼ë¡œ ì €ì¥
    save_processed_data(
        OUTPUT_DIRECTORY,
        X_train, X_val, X_test,
        y_train, y_val, y_test,
        scaler_obj
    )

    print("\nğŸ‰ ì „ì²´ ì „ì²˜ë¦¬ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ê²°ê³¼ íŒŒì¼ë“¤ì´ '{OUTPUT_DIRECTORY}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")